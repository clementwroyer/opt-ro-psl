{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuYvjp81BDPA"
   },
   "source": [
    "### <span style=\"color:rgb(237,127,16)\">Optimisation-RO</span>\n",
    "\n",
    "### <span style=\"color:rgb(237,127,16)\">Certificat chef de projet IA, Université Paris Dauphine-PSL</span>\n",
    "\n",
    "\n",
    "# <span style=\"color:rgb(237,127,16)\">Lab numéro 1 - Bases de l'optimisation douce/lisse</span>\n",
    "\n",
    "Pour tout commentaire concernant ce notebook (y compris les typos), merci d'envoyer un mail à **clement.royer@dauphine.psl.eu**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VX8zRMagBDPJ"
   },
   "source": [
    "# <span style=\"color:rgb(237,127,16)\">Introduction</span>\n",
    "\n",
    "Dans ce notebook, nous allons explorer différents algorithmes pour l'optimisation douce ou lisse, basés sur la descente de gradient. \n",
    "\n",
    "La première partie de ce notebook s'intéresse à un problème de régression linéaire sur lequel on applique l'algorithme de descente de gradient. Dans la seconde, on illustre un résultat récente de convergence presque sûre de la descente de gradient vers un minimum local. Enfin, dans une dernière partie, on se penche sur un problème matriciel non convexe est étudié.\n",
    "\n",
    "\n",
    "*Note : Si les différentes parties de ce notebook sont construites de sorte à être indépendantes, les blocs sont fait pour être exécutés de manière séquentielle. On prendra notamment soin d'exécuter le bloc ci-dessous (qui importe des bibliothèques et fonctions utiles) en premier.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "au80GpvsBDPK"
   },
   "outputs": [],
   "source": [
    "# Imports des bibliothèques et fonctions utiles\n",
    "###############################################\n",
    "\n",
    "# Affichage\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import sqrt # Racine carrée\n",
    "\n",
    "# NumPy - Structures vectorielles et matricielles\n",
    "import numpy as np # NumPy library\n",
    "from numpy.random import multivariate_normal, randn, uniform, choice # distributions de probabilité\n",
    "\n",
    "# SciPy - Calculs mathématiques efficaces\n",
    "from scipy.linalg import norm # normes classiques\n",
    "from scipy.linalg.special_matrices import toeplitz # matrices de Toeplitz\n",
    "from scipy.linalg import svdvals # décomposition en valeurs singulières\n",
    "from scipy.optimize import check_grad # Vérification des dérivées\n",
    "from scipy.optimize import fmin_l_bfgs_b # Méthode de minimisation efficace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook utilise des routines des bibliothèques NumPy and Scipy *(sur Google Colab par défaut, à installer sur votre distribution locale).* Voici un lien vers un bon [tutoriel Numpy (en anglais](https://sebastianraschka.com/pdf/books/dlb/appendix_f_numpy-intro.pdf).\n",
    "\n",
    "**Fonctions NumPy utiles (cf documentation pour plus d'informations)**\n",
    "\n",
    "* *transpose* transposée de matrice (i.e. tableau NumPy bi-dimensionnel). On peut aussi utiliser X.T\n",
    "* *matmul* produit matrice-matrice (les dimensions doivent être compatibles).\n",
    "* *dot* produit matrice-vector (si les dimensions sont compatibles), aussi utilisable comme opérateur de produit scalaire entre deux vecteurs de même taille.\n",
    "* *np.ones((m,n))* matrice de taille m x n avec composantes égales à 1. \n",
    "* *np.zeros((m,n))* matrice de taille m x n avec composantes égales à 0.\n",
    "* *np.identity(n)* matrice identité de taille n x n.\n",
    "* *np.pi* $\\pi$.\n",
    "* *np.inf* nombre infini en mémoire.\n",
    "* *np.log* logarithme appliqué à chaque coordonnée des tableaux NumPy.\n",
    "* *np.exp* exponentielle appliquée à chaque coordonnée des tableaux NumPy.\n",
    "* *np.sum* somme des composantes d'un tableau NumPy (pour les matrices, somme selon une dimension)\n",
    "* *np.maximum(u,v)* renvoie un tableau NumPy array dont les composantes sont $max(u_i,v_i)$, si $u_i$ et $v_i$ sont celles de $u$ et $v$.\n",
    "* *np.concatenate* concatène des tableaux NumPy (vecteurs, matrices) de dimensions compatibles.\n",
    "* Si t est un tableau NumPy, la fonction *t.shape* renvoie les dimension(s) de ce tableau (utile lorsque l'on cherche à définir un tableau avec les mêmes dimensions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:rgb(199,21,11)\">Partie 1 - Descente de gradient et moindres carrés</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(199,21,11)\">1.1 Génération des données</span>\n",
    "\n",
    "Soit un jeu de données $\\{(\\mathbf{x}_i,y_i)\\}_{i=1,\\dots,n}$, où $\\mathbf{x}_i \\in \\mathbb{R}^d$ et $y_i \\in \\mathbb{R}$, que l'on met sous la forme\n",
    "\n",
    "- d'une matrice de caractéristiques (features) $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$;\n",
    "- et d'un vecteur de labels $\\mathbf{y} \\in \\mathbb{R}^n$. \n",
    "\n",
    "Le jeu de données sera produit en utilisant la procédure ci-dessous, qui vise à générer des échantillons avec \n",
    "bruit et corrélation contrôlés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ce code est bassé sur un générateur proposé par Alexandre Gramfort de l'INRIA.\n",
    "def simu_lin(w, n, std=1., corr=0.5):\n",
    "    \"\"\"\n",
    "    Génération de données issues d'un modèle linéaire.\n",
    "    \n",
    "    Paramètres\n",
    "    ----------\n",
    "    w : np.ndarray, shape=(d,)\n",
    "        Coefficients du modèle\n",
    "    \n",
    "    n : int\n",
    "        Taille de l'échantillon\n",
    "    \n",
    "    std : float, default=1.\n",
    "        Ecart-type du bruit\n",
    "\n",
    "    corr : float, default=0.5\n",
    "        Corrélation de la matrice des caractéristiques\n",
    "    \"\"\"    \n",
    "    d = w.shape[0]\n",
    "    cov = toeplitz(corr ** np.arange(0, d))\n",
    "    X = multivariate_normal(np.zeros(d), cov, size=n)\n",
    "    noise = std * randn(n)\n",
    "    y = X.dot(w) + noise\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données seront ainsi produites à partir d'une tendance linéaire à laquelle on rajoutera un bruit gaussien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(199,21,11)\">1.2 Modèle linéaire et moindres carrés</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre objectif est de construire un modèle linéaire qui colle au plus près à nos données. Partant de $(\\mathbf{X},\\mathbf{y})$, on cherche ainsi $\\mathbf{w} \\in \\mathbb{R}^d$ tel que $\\mathbf{X} \\mathbf{w} -\\mathbf{y} \\approx 0$.\n",
    "\n",
    "Sans a priori sur la nature du bruit dont les observations sont entachées (ou en supposant que ce bruit est gaussien), on peut formuler notre objectif comme un problème aux moindres carrés linéaires :\n",
    "$$\n",
    "    \\mbox{minimiser}_{\\mathbf{w} \\in \\mathbb{R}^d} \n",
    "    f(\\mathbf{w}) := \\frac{1}{2 n} \\|\\mathbf{X} \\mathbf{w} - \\mathbf{y}\\|^2 \n",
    "    = \\frac{1}{2n} \\sum_{i=1}^n (\\mathbf{x}_i^T \\mathbf{w} - y_i)^2.\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(199,21,11)\">Etude du problème</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On observe que toute valeur inférieure ou égale à $0$ est une borne inférieure sur $f$ puisque $f$ ne prend que des valeurs positives ou nulles. La valeur $0$ *peut être* la valeur optimale (càd $0 = \\min_{\\mathbf{w} \\in \\mathbb{R}^d} f(\\mathbf{w})$) mais cela dépendra de s'il existe ou non un vecteur $\\mathbf{w}^*$ tel que $\\mathbf{X} \\mathbf{w}^* = \\mathbf{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Le problème peut se réécrire comme un *problème d'optimisation quadratique* de la forme\n",
    "$$\n",
    "    \\mbox{minimiser}_{\\mathbf{w} \\in \\mathbb{R}^d} \\frac{1}{2}\\mathbf{w}^T \\mathbf{A} \\mathbf{w} - \\mathbf{b}^T \\mathbf{w} + c.\n",
    "$$\n",
    "avec $\\mathbf{A} = \\tfrac{\\mathbf{X}^T \\mathbf{X}}{n}$, $\\mathbf{b} = \\mathbf{X}^T \\mathbf{y}$ et $c=\\frac{\\mathbf{y}^T \\mathbf{y}}{n}$.\n",
    "\n",
    "- La fonction objectif est de classe $\\mathcal{C}^1$ (car polynomiale en chacune des variables), et son gradient est donné par\n",
    "$$\n",
    "    \\nabla f(\\mathbf{w}) \n",
    "    = \\mathbf{A} \\mathbf{w} - \\mathbf{b} \n",
    "    = \\frac{1}{n}\\left( \\mathbf{X}^T \\mathbf{X} \\mathbf{w} - \\mathbf{X}^T \\mathbf{y}\\right).\n",
    "$$\n",
    "De plus, pour tous $(\\mathbf{v},\\mathbf{w})$, on a\n",
    "$$\n",
    "    \\| \\nabla f(\\mathbf{v}) - \\nabla f(\\mathbf{w}) \\| \n",
    "    \\le \\| \\mathbf{A} (\\mathbf{v}-\\mathbf{w}) \\| \n",
    "    \\le \\|\\mathbf{A} \\| \\|\\mathbf{v} -\\mathbf{w}\\|,\n",
    "$$\n",
    "où la dernière inégalité provient de l'inégalité de Cauchy-Schwarz et $\\|\\mathbf{A}\\| = \\max_{\\mathbf{u} \\neq 0} \\tfrac{\\|\\mathbf{A}\\mathbf{u}\\|}{\\|\\mathbf{u}\\|}$. Par conséquent, la fonction $f$ est de classe $\\mathcal{C}^{1,1}_{\\|\\mathbf{A}\\|}$, avec $\\|\\mathbf{A}\\| = \\tfrac{1}{n}\\|\\mathbf{X}^T \\mathbf{X}\\|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(199,21,11)\">Classe Python pour le problème de régression linéaire</span>\n",
    "\n",
    "Le code ci-dessous génère des instances de problèmes de régression linéaire. En utilisant les formules ci-dessous, \n",
    "**compléter le code pour calculer $f(\\mathbf{w})$ et $\\nabla f(\\mathbf{w})$ via des méthodes de la classe.*  \n",
    "\n",
    "*Note :* Pour tout tableau NumPy X, X.dot(v) est le produit matrice-vecteur $X v$ si $X$ est une matrice, ou le produit scalaire $x^T v$ s'il s'agit d'un vecteur, et X.T renvoie la matrice transposée $X^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe Python pour les problèmes de régression linéaire\n",
    "class LinReg(object):\n",
    "    '''\n",
    "        Problèmes de régression linéaire sous forme de moindres carrés linéaires.\n",
    "        \n",
    "        Attributs:\n",
    "            X: Tableau à deux dimensions, matrice de données (caractéristiques)\n",
    "            y: Tableau à une dimension, vecteur de données (labels/mesures/...)\n",
    "            n,d: Dimensions du problème (X de taille n x d, y est de taille n)\n",
    "            \n",
    "        Méthodes:\n",
    "            fun: Calcule la valeur de la fonction objectif pour les moindres carrés linéaires.\n",
    "            grad: Calcule la valeur du gradient pour les moindres carrés linéaires.\n",
    "            lipgrad: Calcule la valeur de la constante de Lipschitz pour le gradient.\n",
    "    '''   \n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n, self.d = X.shape\n",
    "    \n",
    "    # Fonction objectif\n",
    "    def fun(self, w):\n",
    "        ## COMPLETER LE CODE AVEC LA FORMULE DE LA FONCTION OBJECTIF\n",
    "        return  \n",
    "    \n",
    "    # Gradient\n",
    "    def grad(self, w):\n",
    "        ## COMPLETER LE CODE AVEC LA FORMULE DU GRADIENT\n",
    "        return \n",
    "\n",
    "    # Constante de Lipschitz pour le gradient\n",
    "    def lipgrad(self):\n",
    "        L = norm(self.X, ord=2) ** 2 / self.n # Calcul plus économe de ||X^T X||\n",
    "        return L "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(199,21,11)\">Génération du problème</span>\n",
    "\n",
    "**Valider l'implémentation ci-dessus en générant une instance.** L'appel à la fonction *check_grad*, qui vérifie la correspondance entre une fonction et sa dérivée, doit renvoyer une valeur de l'ordre de $10^{-6}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions \n",
    "d = 50\n",
    "n = 1000\n",
    "idx = np.arange(d)\n",
    "\n",
    "# Fixer la configuration du generateur de nombres aleatoires\n",
    "np.random.seed(1)\n",
    "\n",
    "# Vérité terrain - Les coefficients de w décroissent de manière exponentielle\n",
    "w_model_truth = (-1)**idx * np.exp(-idx / 10.)\n",
    "\n",
    "# Génération de la matrice et du vecteur de données\n",
    "X, y = simu_lin(w_model_truth, n, std=1., corr=0.1)\n",
    "\n",
    "# Génération de l'instance correspondante\n",
    "pblinreg = LinReg(X, y)\n",
    "\n",
    "# Vérification des formules, et de la cohérence fonction/gradient<\n",
    "# Si correct, renvoie une valeur de l'ordre de 10^(-6) \n",
    "check_grad(pblinreg.fun, pblinreg.grad, np.random.randn(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On travaille ici avec un problème de dimensions modérées, ce qui nous permet de calculer une solution quasi exacte au moyen d'une méthode d'optimisation non linéaire avancée (L-BFGS dans notre cas) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On utilise L-BFGS afin de trouver une approximation précise de la solution au problème\n",
    "\n",
    "w_init = np.zeros(d)\n",
    "w_min_lin, f_min_lin, _ = fmin_l_bfgs_b(pblinreg.fun, w_init, pblinreg.grad, args=(), pgtol=1e-30, factr =1e-30)\n",
    "print(f_min_lin)\n",
    "print(norm(pblinreg.grad(w_min_lin)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(199,21,11)\">1.2 Descente de gradient</span>\n",
    "\n",
    "L'algorithme de descente de gradient appliqué à une fonction $f$ est défini par un point initial $w_0$ ainsi que \n",
    "par l'itération\n",
    "$$\n",
    "\\mathbf{w}_{k+1} = \\mathbf{w}_k - \\alpha_k \\nabla f(\\mathbf{w}_k),\n",
    "$$\n",
    "\n",
    "**Compléter le squelette ci-dessous pour implémenter la descente de gradient avec trois choix de longueurs de pas possibles:**\n",
    "\n",
    "- $\\alpha_k = \\frac{1}{L}$, avec $L$ la constante de Lipschitz pour $\\nabla f$;\n",
    "\n",
    "- $\\alpha_k = \\frac{\\alpha_0}{k+1}$;\n",
    "\n",
    "- $\\alpha_k = \\frac{\\alpha_0}{\\sqrt{k+1}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descente de gradient\n",
    "def grad_desc(w0,problem,wtarget,stepchoice=0,step0=1, n_iter=1000, verbose=False): \n",
    "    \"\"\"\n",
    "        Un code pour la descente de gradient sur problèmes structurés avec différents choix de \n",
    "        longueurs de pas.\n",
    "        \n",
    "        Entrées:\n",
    "            w0: Vecteur initial\n",
    "            problem: Structure représentant le problème\n",
    "                problem.fun(w) Fonction objectif\n",
    "                problem.grad(w) Gradient\n",
    "                problem.lipgrad() Constante de Lipschitz du gradient\n",
    "            wtarget: Valeur cible pour le minimum\n",
    "            stepchoice: Stratégie pour la longueur de pas (see above)\n",
    "                0: Constante égale à 1/L\n",
    "                1: Décroissante en 1/(k+1)\n",
    "                2: Décroissante en 1/sqrt(k+1)\n",
    "            step0: Longueur de pas initiale (utile si stepchoice = 1)\n",
    "            n_iter: Nombre maximum d'itérations\n",
    "            verbose: Booléen réglant l'affichage des informations à chaque itération\n",
    "      \n",
    "        Sorties:\n",
    "            w_output: Itéré final de la méthode\n",
    "            objvals: Historique des valeurs de fonctions (tableau Numpy de taille n_iter)\n",
    "            normits: Historique des distances entre itérés et point cible (tableau Numpy de taille n_iter)\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    ############\n",
    "    # Initialisation\n",
    "\n",
    "    # Historique des valeurs\n",
    "    objvals = []\n",
    "    \n",
    "    # Distances entre itérés et point cible\n",
    "    normits = []\n",
    "    \n",
    "    # Constante de Lipschitz\n",
    "    L = problem.lipgrad()\n",
    "    \n",
    "    # Valeur initiale de l'itéré courant   \n",
    "    w = w0.copy()\n",
    "\n",
    "    # Initialisation du compteur d'itérations\n",
    "    k=0    \n",
    "    \n",
    "    # Fonction objectif en le point courant\n",
    "    obj = problem.fun(w) \n",
    "    objvals.append(obj);\n",
    "    # Distance courante à l'optimum\n",
    "    nmin = norm(w-wtarget)\n",
    "    normits.append(nmin)\n",
    "\n",
    "    # Affichage des valeurs initiales \n",
    "    if verbose:\n",
    "        print(\"Descente de gradient :\")\n",
    "        print(' | '.join([name.center(8) for name in [\"iter\", \"fval\", \"dist\"]]))\n",
    "        print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % nmin).rjust(8)]))\n",
    "    \n",
    "    ####################\n",
    "    # Boucle principale\n",
    "    while (k < n_iter):\n",
    "        \n",
    "        # A COMPLETER\n",
    "        # Calcul du gradient\n",
    "        g = \n",
    "        \n",
    "        # Choix de la longueur de pas\n",
    "        if stepchoice==0:\n",
    "            w[:] = \n",
    "        elif stepchoice==1:\n",
    "            w[:] = \n",
    "        else:\n",
    "            w[:] = \n",
    "        # FIN A COMPLETER\n",
    "        \n",
    "        \n",
    "        # Calcul des nouvelles informations\n",
    "        obj = problem.fun(w)\n",
    "        objvals.append(obj)\n",
    "        nmin = norm(w-wtarget)\n",
    "        normits.append(nmin)\n",
    "        \n",
    "        # Affichage si demandé\n",
    "        if verbose:\n",
    "            print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % nmin).rjust(8)]))       \n",
    "        \n",
    "        # Incrémentation\n",
    "        k += 1\n",
    "    \n",
    "    # Fin boucle principale\n",
    "    ######################\n",
    "    \n",
    "    # Informations de sortie\n",
    "    w_output = w.copy()\n",
    "    return w_output, np.array(objvals), np.array(normits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparer les trois variantes en faisant tourner les scripts ci-dessous.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester trois variantes de descente de gradient\n",
    "w0 = np.zeros(d)\n",
    "w_a, obj_a, nits_a = grad_desc(w0,pblinreg,w_min_lin,stepchoice=0,step0=1, n_iter=100)\n",
    "w_b, obj_b, nits_b = grad_desc(w0,pblinreg,w_min_lin,stepchoice=1,step0=1, n_iter=100)\n",
    "w_c, obj_c, nits_c = grad_desc(w0,pblinreg,w_min_lin,stepchoice=2,step0=1, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison de trois variantes de descente de gradient \n",
    "\n",
    "# En termes de fonction objectif\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(obj_a-f_min_lin, label=\"GD - 1/L\", lw=2)\n",
    "plt.semilogy(obj_b-f_min_lin, label=\"GD - 1/(k+1)\", lw=2)\n",
    "plt.semilogy(obj_c-f_min_lin, label=\"GD - 1/(sqrt(k+1))\", lw=2)\n",
    "plt.title(\"Convergence\", fontsize=16)\n",
    "plt.xlabel(\"#itérations\", fontsize=14)\n",
    "plt.ylabel(\"Objectif (log)\", fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "# En termes de distance à l'optimum\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(nits_a, label=\"GD - 1/L\", lw=2)\n",
    "plt.semilogy(nits_b , label=\"GD - 1/(k+1)\", lw=2)\n",
    "plt.semilogy(nits_c, label=\"GD - 1/(sqrt(k+1))\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#itérations\", fontsize=14)\n",
    "plt.ylabel(\"Distance à l'optimum (log)\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQNBOKzcBDPl"
   },
   "source": [
    "# <span style=\"color:rgb(92,29,79)\"> Partie 2 - Descente de gradient et minima locaux</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGrLlXgDBDPm"
   },
   "source": [
    "Dans cette partie, notre but est d'observer un résultat récente (établi en 2015) concernant la convergence de la descente de gradient. Il a en effet été démontré que la descente de gradient peut s'échapper de points stationnaires d'ordre 1 qui ne vérifient pas la condition d'optimalité à l'ordre deux **presque sûrement**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ujQoSBuBDPn"
   },
   "source": [
    "## <span style=\"color:rgb(92,29,79)\">2.1 Problème quadratique</span>\n",
    "\n",
    "On considère le problème\n",
    "$$\n",
    "    \\mbox{minimiser}_{\\mathbf{w} \\in \\mathbb{R}^d}\\ q(\\mathbf{w}):=\\frac{1}{2} \\mathbf{w}^T \\mathbf{A} \\mathbf{w}\n",
    "$$\n",
    "où $\\mathbf{A}$ est une matrice diagonale avec entrées $(1,\\dots,1,-\\lambda)$ sur la diagonale et $\\lambda \\in (0,1]$. On a alors $\\|\\mathbf{A}\\|=1$.\n",
    "\n",
    "### <span style=\"color:rgb(92,29,79)\">Etude du problème</span>\n",
    "\n",
    "- Ce problème est non bornée. En posant $\\mathbf{w}=t\\mathbf{e}_d$ (dernier vecteur de la base canonique dans $\\mathbb{R}^d$) pour $t \\in \\mathbb{R}$, on obtient\n",
    "$$\n",
    "    q(\\mathbf{w}) = -\\frac{t^2}{2}\\lambda \\rightarrow -\\infty \\quad \\mbox{lorsque} \\quad t \\rightarrow \\pm \\infty.\n",
    "$$\n",
    "La fonction n'a donc pas de minimum.\n",
    "- La matrice hessienne de $q$ est $\\nabla^2 q(\\mathbf{w})=\\mathbf{A}$, dont les valeurs propres sont $1$ and $-\\lambda<0$. Par conséquent, $\\mathbf{A}$ n'est pas semi-définie positive, et le problème est non convexe.\n",
    "- Pour tout $\\mathbf{w} \\in \\mathbb{R}^d$, on a \n",
    "$$\n",
    "    \\nabla q(\\mathbf{w}) = \\mathbf{A} \\mathbf{w}.\n",
    "$$\n",
    "et le problème est de classe $\\mathcal{C}^{1,1}_{\\|\\mathbf{A}\\|}$.\n",
    "- Le point $\\mathbf{w}=\\mathbf{0}$ vérifie $\\nabla q(\\mathbf{w})=\\mathbf{0}$, mais sa matrice hessienne n'est pas semi-définie positive. Il s'agit donc d'un point stationnaire à l'ordre 1 mais pas à l'ordre 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compléter le code ci-dessous pour afficher la fonction en question.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DoYoIAUiBDPr",
    "outputId": "1d5869a4-d1a3-453e-dfc2-19ed5c14795e"
   },
   "outputs": [],
   "source": [
    "# Define relevant quantities\n",
    "d = 2\n",
    "lbda = 0.01\n",
    "\n",
    "# A COMPLETER : DEFINIR LA MATRICE A ET LES FONCTIONS ASSOCIEES\n",
    "A = \n",
    "\n",
    "toy_f = lambda w :  # Fonction objectif\n",
    "toy_g = lambda w :  # Gradient\n",
    "\n",
    "# FIN A COMPLETER\n",
    "\n",
    "\n",
    "# Plot countours of the objective function if the dimension allows it\n",
    "if d==2:\n",
    "    delta = 5\n",
    "    w1 = np.linspace(-1,1,100)\n",
    "    w2 = np.linspace(-delta,delta,100)\n",
    "    fw = [[toy_f(np.array([u1,u2])) for u1 in w1] for u2 in w2]\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.contour(w1,w2,fw,levels=30)\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLfcNV_NBDPr"
   },
   "source": [
    "## <span style=\"color:rgb(92,29,79)\">2.2 Application de la descente de gradient</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On considère une version simplifiée du code de descente de gradient de la partie 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwpZRLuSBDPm"
   },
   "outputs": [],
   "source": [
    "def gd(w0,fun,gfun,step,n_iter=100, verbose=False): \n",
    "    \"\"\"\n",
    "        Code basique de descente de gradient\n",
    "        \n",
    "        Entrees:\n",
    "            w0: Vecteur initial\n",
    "            fun: Fonction objectif\n",
    "            gfun: Vecteur gradient pour la fonction objectif\n",
    "            step: Valeur de la longueur de pas\n",
    "            n_iter: Nombre d'itérations\n",
    "            verbose: Booléen réglant l'affichage des informations à chaque itération\n",
    "      \n",
    "        Sorties:\n",
    "            w_output: Itéré final de la méthode\n",
    "            objvals: Historique des valeurs de fonctions (tableau Numpy de taille n_iter)\n",
    "            ngvals: Historique des normes de gradient (tableau Numpy de taille n_iter)\n",
    "            \n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    ############\n",
    "    # Initialisation\n",
    "\n",
    "    # Historique des valeurs de fonction\n",
    "    objvals = []\n",
    "    \n",
    "    # Historique des normes de gradient\n",
    "    ngvals = []\n",
    "    \n",
    "    # Valeur initiale de l'itéré courant   \n",
    "    w = w0.copy()\n",
    "\n",
    "    # Indice d'itération\n",
    "    k=0    \n",
    "    \n",
    "    # Objectif au point initial\n",
    "    obj = fun(w)\n",
    "    objvals.append(obj)\n",
    "    \n",
    "    # Gradient au point initial\n",
    "    g = gfun(w)\n",
    "    ngrad = norm(g)\n",
    "    ngvals.append(ngrad)\n",
    "\n",
    "    # Affiche\n",
    "    if verbose:\n",
    "        print(\"Descente de gradient :\")\n",
    "        print(' | '.join([name.center(8) for name in [\"iter\", \"fval\", \"ngrad\"]]))\n",
    "        print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % ngrad).rjust(8)]))\n",
    "    \n",
    "    ####################\n",
    "    # Boucle principale\n",
    "    while (k < n_iter):\n",
    "        \n",
    "        # Calcul du gradient\n",
    "        g = gfun(w)\n",
    "        \n",
    "        # Choix de la longueur de pas et mise à jour\n",
    "        w[:] = w - step * g\n",
    "        \n",
    "        # Calcul de la nouvelle valeur d'objectif\n",
    "        obj = fun(w)\n",
    "        objvals.append(obj)\n",
    "        \n",
    "        # Calcul du nouveau gradient\n",
    "        g = gfun(w)\n",
    "        ngrad = norm(g)\n",
    "        ngvals.append(ngrad)\n",
    "        \n",
    "        # Affichage\n",
    "        if verbose:\n",
    "            print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % ngrad).rjust(8)]))       \n",
    "        \n",
    "        # Incrémenter le nombre d'itérations de 1\n",
    "        k += 1\n",
    "    \n",
    "    # Fin boucle principale\n",
    "    ######################\n",
    "    \n",
    "    # Sorties\n",
    "    w_output = w.copy()\n",
    "    return w_output, np.array(objvals), np.array(ngvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-RwMaUYBDPr"
   },
   "source": [
    "Le script ci-dessous génére des points au hasard et observe si la fonction échappe au voisinage de l'origine (on \n",
    "considèrera que cela est le cas lorsque $f(\\mathbf{w})<f(0)=0$.\n",
    "\n",
    "- Il est possible de définir des points pour lesquels l'algorithme va converger vers l'origine. Il suffit en effet de considérer un vecteur dans l'espace engendré par les $d-1$ premiers vecteurs coordonnées : $\\mathbf{w} \\in \\mbox{vect}\\{\\mathbf{e}_1,\\dots,\\mathbf{e}_{d-1}\\}$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBVobMc7BDPs"
   },
   "source": [
    "**Compléter le code ci-dessous pour ajouter un tel point problématique, et valider l'implémentation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQFumdnfBDPt",
    "outputId": "ee3483a4-7132-4eb9-e28e-3c7d6e543f27",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Tester un ensemble de points initiaux au hasard\n",
    "\n",
    "ntrials = 10\n",
    "\n",
    "step = 1\n",
    "\n",
    "# Booléen pour l'ajout d'un point problématique\n",
    "#failpt = False\n",
    "failpt = True\n",
    "\n",
    "# Initialisation de structures\n",
    "if failpt:\n",
    "    Wf = np.zeros((ntrials+1,d))\n",
    "    vf = np.zeros(ntrials+1)\n",
    "else:\n",
    "    Wf = np.zeros((ntrials,d))\n",
    "    vf = np.zeros(ntrials)\n",
    "\n",
    "# Compteur du nombre de bonnes exécutions (pour lesquelles on esquive l'origine)\n",
    "goodtrials = 0\n",
    "\n",
    "# Lancement des instances de l'algorithme\n",
    "for i in range(ntrials):\n",
    "    w0 = uniform([-1,-1],[1,1],size=d)\n",
    "    Wf[i,:], obj_q, ngrad_q = gd(w0,toy_f,toy_g,step,100)\n",
    "    vf[i] = obj_q[-1]\n",
    "    if vf[i]<0:\n",
    "        goodtrials += 1\n",
    "        \n",
    "# Ajout d'un point problématique pour la descente de gradient\n",
    "if failpt:\n",
    "    # A COMPLETER\n",
    "    w0 = \n",
    "    # FIN A COMPLETER\n",
    "    Wf[ntrials,:], obj_q, ngrad_q = gd(w0,toy_f,toy_g,step,100)\n",
    "    vf[ntrials] = obj_q[-1]\n",
    "    if vf[ntrials]<0:\n",
    "        goodtrials +=1\n",
    "\n",
    "if failpt:\n",
    "    print('La descente de gradient esquive le point selle ',goodtrials,' fois sur ', ntrials+1)\n",
    "else:\n",
    "    print('La descente de gradient esquive le point selle ',goodtrials,' fois sur ', ntrials)\n",
    "    \n",
    "# Affichage des resultats (uniquement en dimension 2)\n",
    "if d==2:\n",
    "    npts = vf.size\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.contour(w1,w2,fw,levels=30)\n",
    "    plt.colorbar()\n",
    "    for i in range(npts):\n",
    "        if vf[i]<0:\n",
    "            plt.plot(Wf[i,0],Wf[i,1],'o',color='green')\n",
    "        else:\n",
    "            plt.plot(Wf[i,0],Wf[i,1],'o',color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQ-guT7YBDPO"
   },
   "source": [
    "# <span style=\"color:rgb(237,127,16)\">Partie 3 (Bonus) - Descente de gradient et optimisation non convexe</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wjr-DiDkBDPP"
   },
   "source": [
    "Dans cette dernière partie, nous allons appliquer l'algorithme de descente de gradient à une formulation \n",
    "non convexe issue des problèmes de complétion de matrice de rang faible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqVFvgHSBDPQ"
   },
   "source": [
    "## <span style=\"color:rgb(237,127,16)\">3.1 Données</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-uuVXKABDPT"
   },
   "source": [
    "On considère une matrice $\\mathbf{M} \\in \\mathbb{R}^{d \\times d}$ que l'on suppose symétrique semi-définie positive et *de rang faible* : cela signifie que $\\mathbf{M}$ ne possède qu'un faible nombre $r \\ll d$ de valeurs propres strictement positives et que toutes les autres sont nulles. Une telle matrice peut s'écrire sous la forme $\\mathbf{M}=\\mathbf{U}\\mathbf{U}^T$ avec $\\mathbf{U} \\in \\mathbb{R}^{d \\times r}$\n",
    "\n",
    "Notre but est de déterminer une approximatin de $\\mathbf{M}$ au moyen d'un nombre limité d'observations $s$, produites par le code ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R97_WTgSBDPY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Génération d'une matrice aléatoire de rang faible et échantillonnage\n",
    "# Ce code est adapté d'un exercice proposé par Irène Waldspurger.\n",
    "\n",
    "def datamatrix(d,r,nb_s):\n",
    "    '''\n",
    "        Génération d'une matrice aléatoire de rang faible et d'une version \n",
    "        échantillonnée de celle-ci.\n",
    "        \n",
    "        Entrées\n",
    "            d: Dimension de la matrice (on considère des matrices de taille d x d\n",
    "            r: Rang de la matrice\n",
    "            nb_s: Nombre d'entrées échantillonnées\n",
    "        \n",
    "        Sortie:\n",
    "            M: Matrice de données de taille d x d et de rang r\n",
    "            S: Masque définissant les échantillons, matrice de taille d x d à coefficients dans {0,1}\n",
    "            M_sampled: Matrice sous-échantillonnée\n",
    "            U: Facteur ayant généré la matrice\n",
    "    '''\n",
    "\n",
    "    # Véritable facteur et matrice\n",
    "    U = randn(d,r)\n",
    "    M = np.dot(U,np.transpose(U))\n",
    "\n",
    "    # Génération de la version sous-échantillonnée\n",
    "    subset = choice(d*d,nb_s,replace=False)\n",
    "    S = np.zeros(d*d)\n",
    "    S[subset] = 1\n",
    "    S = S.reshape((d,d))\n",
    "    M_sampled = M*S # Produit de Hadamard pour NumPy\n",
    "\n",
    "    return M,S,M_sampled,U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "V9nUvJtBBDPa",
    "outputId": "1828a620-fd60-41f3-b15e-86945f073c94"
   },
   "outputs": [],
   "source": [
    "d = 5 # Taille de la matrice d*d\n",
    "r = 2 # Rang\n",
    "nb_s = 13 # Nombre d'entrées observées\n",
    "\n",
    "# Paramètre du générateur de nombres aléatoires\n",
    "np.random.seed(1)\n",
    "\n",
    "M,S,M_sampled,U = datamatrix(d,r,nb_s)\n",
    "\n",
    "\n",
    "# Affichage\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.get_cmap('twilight')\n",
    "plt.subplot(131)\n",
    "plt.title('Vérité terrain')\n",
    "plt.imshow(M)\n",
    "plt.colorbar()\n",
    "plt.subplot(132)\n",
    "plt.title('Echantillonnée')\n",
    "plt.imshow(M_sampled)\n",
    "plt.colorbar()\n",
    "plt.subplot(133)\n",
    "plt.title('Masque')\n",
    "plt.imshow(S)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH-awA01BDPd"
   },
   "source": [
    "## <span style=\"color:rgb(237,127,16)\">3.2 Formulations non convexes et dérivées</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DixHYwRMBDPd"
   },
   "source": [
    "On formule maintenant le problème de manière non convexe. Sachant que la solution s'exprime comme $\\mathbf{U}\\mathbf{U}^T$, on va cherche à approcher ce facteur plutôt que la matrice $\\mathbf{M}$ elle-même (ce qui permet notamment de s'affranchir d'une formulation avec contraintes, et de manipuler moins de variables). On obtient ainsi le problème suivant :\n",
    "$$\n",
    "    \\mathrm{minimiser}_{\\mathbf{U} \\in \\mathbb{R}^{d \\times r}} f(\\mathbf{U}):=\\frac{1}{2}\\sum_{(i,j) \\in \\mathcal{S}} \\left( [\\mathbf{U}\\mathbf{U}^T]_{ij}-\\mathbf{M}_{ij}\\right)^2.\n",
    "$$\n",
    "où $\\mathcal{S} \\subset \\{1,\\dots,d\\}^2$ représente l'ensemble des entrées de la matrice de données $\\mathbf{M}$ qui sont observées. Ce problème est sans contraintes sur ses $dr$ variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0KvLLBsBDPf"
   },
   "source": [
    "### <span style=\"color:rgb(237,127,16)\">Approche matricielle</span>\n",
    "\n",
    "Plutôt que d'effectuer une descente de gradient standard basée sur un vecteur de variables, on va définir le gradient de $f$ par rapport à la matrice $\\mathbf{U}$, qui sera donc une matrice de $\\mathbb{R}^{d \\times r}$ (de \n",
    "même que dans le cas vectoriel, où $\\nabla \\phi(\\mathbf{x}) \\in \\mathbb{R}^d$ pour toute fonction $\\phi: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ et $\\mathbf{x} \\in \\mathbb{R}^d$). Dans notre cas, le gradient est défini ainsi :\n",
    "$$\n",
    "    \\forall \\mathbf{U} \\in \\mathbb{R}^{d \\times r}, \\qquad \n",
    "    \\nabla f(\\mathbf{U}) := (\\mathbf{E}+\\mathbf{E}^T)\\mathbf{U} \\quad \\mbox{where} \\quad \n",
    "    \\mathbf{E}:= (\\mathbf{U}\\mathbf{U}^T-\\mathbf{M}) \\otimes \\mathbf{S},\n",
    "$$\n",
    "avec $\\otimes$ le produit de Hadamard (produit des coefficients) et $\\mathbf{S} \\in \\mathbb{R}^{d \\times d}$ définie par\n",
    "$$\n",
    "    \\mathbf{S}_{ij} = \n",
    "    \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1 &\\mbox{si $(i,j) \\in \\mathcal{S}$} \\\\\n",
    "            0 &\\mbox{sinon.}\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "*NB: On a $\\nabla f(\\mathbf{0})=\\mathbf{0}$, la matrice nulle est donc un point stationnaire du problème.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlLn9qn-BDPg"
   },
   "source": [
    "**Compléter l'implémentation des deux fonctions suivantes**\n",
    "- Une fonction *f_mat* qui calcule la valeur de l'objectif;\n",
    "- Une fonction *fg_mat* qui renvoie à la fois la valeur de l'objectif et celle du gradient matriciel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MV5pSeb1BDPh"
   },
   "outputs": [],
   "source": [
    "# Fonctions du problème\n",
    "def f_mat(U,M_S,S):\n",
    "    '''\n",
    "        Fonction objectif pour le problème de factorisation de matrice de rang faible.\n",
    "        \n",
    "        Entrées:\n",
    "            U: matrice d x r (variables de décision)\n",
    "            M_S: Echantillons de la matrice de données\n",
    "            S: Masque d'échantillonnage\n",
    "            \n",
    "        Sortie:\n",
    "            f: Valeur de la fonctionnelle de rang faible en U\n",
    "    '''\n",
    "    # A COMPLETER\n",
    "    f= \n",
    "    # FIN A COMPLETER\n",
    "    return f\n",
    "\n",
    "\n",
    "def fg_mat(U,M_S,S):\n",
    "    '''\n",
    "        Fonction objectif et vecteur gradient pour le problème de factorisation de matrice de rang faible.\n",
    "        \n",
    "        Entrées:\n",
    "            U: matrice d x r (variables de décision)\n",
    "            M_S: Echantillons de la matrice de données\n",
    "            S: Masque d'échantillonnage\n",
    "            \n",
    "        Sortie:\n",
    "            f: Valeur de la fonctionnelle de rang faible en U\n",
    "            G: Valeur du gradient de la fonctionnelle en U (défini comme une matrice)\n",
    "    '''\n",
    "    # A COMPLETER\n",
    "    f = \n",
    "    G = \n",
    "    # FIN A COMPLETER\n",
    "    return f,G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-hHs7jLBDPi"
   },
   "source": [
    "On implémente maintenant une méthode de descente de gradient spécifique à ce problème. Pour chaque instance, on \n",
    "considère que\n",
    "- l'instance a convergé vers un minimum global si $f(\\mathbf{U}_K \\mathbf{U}_K^T) \\le 10^{-4}\\|\\mathbf{M}_S\\|^2$;\n",
    "- l'instance a convergé vers un point stationnaire si le test précédent échoue mais que \n",
    "$\\nabla f(\\mathbf{U}_K) \\le 10^{-3}\\|\\mathbf{U}_K\\|$;\n",
    "- l'instance a échoué sinon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "042GvD56BDPi"
   },
   "outputs": [],
   "source": [
    "def gd_matrix(U0,fun,fungrad,f_base,step,linesearch=False,n_iter=10000, verbose=False): \n",
    "    \"\"\"\n",
    "        Code de la descente de gradient pour la complétion de matrice de rang faible.\n",
    "        \n",
    "        Entrées:\n",
    "            U0: Matrice initiale (rectangulaire)\n",
    "            fun: Fonction objectif en U\n",
    "            fungrad: Renvoie à la fois la fonction objectif et le gradient\n",
    "            f_base: Valeur de base pour la convergence\n",
    "            step: Valeur initiale de la longueur de pas\n",
    "            linesearch: Booléen indiquant si une recherche linéaire est utilisée\n",
    "            n_iter: Nombre maximum d'itérations\n",
    "            verbose: Booléen d'affichage par itération\n",
    "      \n",
    "        Sorties:\n",
    "            U_output: Itéré final de la méthode\n",
    "            objvals: Historique des valeurs de fonctions (t as a Numpy array of length at most n_iter+1)\n",
    "            ngvals: Historique des normes (output as a Numpy array of length at most n_iter+1)\n",
    "            nits: Nombre d'itérations effectuées\n",
    "            flag: Indicateur d'arrêt\n",
    "                0: Convergence en termes de valeur de fonction (décroît f_base par un facteur 1e-4)\n",
    "                1: Convergence en termes de norme de gradient (en dessous de 1e-3*norm(U_output))\n",
    "                2: Nombre maximum d'itérations atteint\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    ############\n",
    "    # Initialisation\n",
    "\n",
    "    # Historique des fonctions objectifs\n",
    "    objvals = []\n",
    "    \n",
    "    # Historique des normes de gradient\n",
    "    ngvals = []\n",
    "    \n",
    "    # Valeur initiale de l'itéré courant  \n",
    "    U = U0.copy()\n",
    "\n",
    "    # Initialisation de l'indice d'itération\n",
    "    k=0    \n",
    "    \n",
    "    # Valeurs courantes de l'objectif et de son gradient\n",
    "    obj,G = fungrad(U)\n",
    "    objvals.append(obj)\n",
    "    ngrad = norm(G)\n",
    "    ngvals.append(ngrad)\n",
    "\n",
    "    # Affichage\n",
    "    if verbose:\n",
    "        print(\"Descente de gradient :\")\n",
    "        print(' | '.join([name.center(8) for name in [\"iter\", \"fval\", \"ngrad\"]]))\n",
    "        print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % ngrad).rjust(8)]))\n",
    "    \n",
    "    # Critère d'arrêt\n",
    "    if (obj<=1e-4*f_base):\n",
    "        flag=0\n",
    "    elif (ngrad<=1e-3*norm(U)):\n",
    "        flag=1\n",
    "    else:\n",
    "        flag=2\n",
    "        \n",
    "    stopping=(flag==2)\n",
    "    \n",
    "    ####################\n",
    "    # Boucle principale\n",
    "    while stopping:\n",
    "        \n",
    "        # Choix de la longueur de pas\n",
    "        if linesearch:\n",
    "            if k>0:\n",
    "                step = 1.1*step\n",
    "            fval = fun(U-step*G)\n",
    "            ngrad2 = ngrad**2\n",
    "            # Boucle de recherche linéaire pour calculer une longueur de pas\n",
    "            while (fval-obj>-0.5*step*ngrad2):\n",
    "                step = step/2\n",
    "                fval = fun(U-step*G)\n",
    "        \n",
    "        U = U - step * G\n",
    "        \n",
    "        # Calcul de l'objectif et du gradient\n",
    "        obj,G = fungrad(U)\n",
    "        objvals.append(obj)\n",
    "        ngrad = norm(G)\n",
    "        ngvals.append(ngrad)\n",
    "        \n",
    "        # Affichage\n",
    "        if verbose:\n",
    "            print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % ngrad).rjust(8)]))       \n",
    "        \n",
    "        # Incrément de l'indice d'itération\n",
    "        k += 1\n",
    "        \n",
    "        # Critère d'arrêt\n",
    "        if (obj<=1e-4*f_base):\n",
    "            flag=0\n",
    "        elif (ngrad<=1e-3*norm(U)):\n",
    "            flag=1\n",
    "        else:\n",
    "            flag=2\n",
    "        stopping = (flag==2)\n",
    "    \n",
    "    # Fin de la boucle principale\n",
    "    ######################\n",
    "    \n",
    "    # Fin de l'algorithme\n",
    "    U_output = U.copy()\n",
    "    nits = k\n",
    "        \n",
    "    return U_output, np.array(objvals), np.array(ngvals),nits,flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 857
    },
    "id": "t8Tw4kU-BDPj",
    "outputId": "ec101199-0139-416e-9f81-97a16621887f"
   },
   "outputs": [],
   "source": [
    "# Validation sur un petit exemple\n",
    "\n",
    "d=5\n",
    "r=2\n",
    "nb_s=13\n",
    "\n",
    "M,S,M_s,U_star = datamatrix(d,r,nb_s)\n",
    "\n",
    "fbase = 0.5*(np.sum(M_s**2))\n",
    "\n",
    "# Tester la descente de gradient\n",
    "U0 = randn(d,r)\n",
    "#U0 = U_star\n",
    "U_out,obj_out,ng_out,nits,flag = gd_matrix(U0,lambda U: f_mat(U,M_s,S),lambda U: fg_mat(U,M_s,S),fbase,1.,True,10000,False)\n",
    "\n",
    "#print(nits)\n",
    "print(flag)\n",
    "# Résultats finaux\n",
    "print(\"Fonction objectif normalisée:\",obj_out[-1]/(norm(M_s)**2))\n",
    "print(\"Norme de gradient normalisée:\",ng_out[-1]/norm(U_out))\n",
    "M_out = np.dot(U_out,np.transpose(U_out))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.semilogy(obj_out/norm(M_s), lw=2)\n",
    "plt.xlabel('Itérations')\n",
    "plt.ylabel('Objectif (log)')\n",
    "plt.figure(2)\n",
    "plt.semilogy(ng_out, lw=2)\n",
    "plt.xlabel('Itérations')\n",
    "plt.ylabel('Norme de gradient (log)')\n",
    "plt.figure(3)\n",
    "plt.subplot(221)\n",
    "plt.title('Vérité terrain')\n",
    "plt.imshow(M)\n",
    "plt.subplot(222)\n",
    "plt.title('Solution obtenue')\n",
    "plt.imshow(M_out,vmin=np.amin(M_out),vmax=np.amax(M_out))\n",
    "plt.subplot(223)\n",
    "plt.title('Différence')\n",
    "plt.imshow(M-M_out,vmin=np.amin(M_out),vmax=np.amax(M_out))\n",
    "plt.subplot(224)\n",
    "plt.title('Masque')\n",
    "plt.imshow(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oB_mNoDpNtmL"
   },
   "source": [
    "On compare finalement la qualité de la solution obtenue pour différentes valeurs du nombre d'échantillons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "id": "QP2Mb027PHWN",
    "outputId": "ad24df69-7984-45bf-f17b-7edd8bee59f2"
   },
   "outputs": [],
   "source": [
    "# Tester plusieurs instances\n",
    "\n",
    "d=9\n",
    "r=1\n",
    "vals_s = range(10,40,2)\n",
    "len_s = len(vals_s)\n",
    "counter = np.zeros((len_s,3))\n",
    "#\n",
    "for n_it in range(0,len_s):\n",
    "    nb_s = vals_s[n_it]\n",
    "    print(str(nb_s) + ' - '),\n",
    "    for i in range(0,50):\n",
    "        M,S,M_s,U_star = datamatrix(d,r,nb_s)\n",
    "        nMs2 = 0.5*norm(M_s)**2\n",
    "        U0 = randn(d,r)\n",
    "        U_out,obj_out,ng_out,_,flag = gd_matrix(U0,lambda U: f_mat(U,M_s,S),lambda U: fg_mat(U,M_s,S),nMs2,1.,True,20000,False)\n",
    "        counter[n_it,flag]+=1\n",
    "#\n",
    "plt.xlabel('Nombre échantillons (sur un total de '+str(d**2)+')')\n",
    "plt.ylabel('Nombre instances')\n",
    "plt.plot(vals_s,counter[:,0],label='Global minimum')\n",
    "plt.plot(vals_s,counter[:,1],label='Point stationnaire')\n",
    "plt.plot(vals_s,counter[:,2],label='Echec')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zmv9hngBDPv"
   },
   "outputs": [],
   "source": [
    "## Version 1.0 - C. W. Royer, novembre 2021."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "kuYvjp81BDPA",
    "VX8zRMagBDPJ",
    "dQNBOKzcBDPl",
    "1ujQoSBuBDPn",
    "KLfcNV_NBDPr"
   ],
   "name": "LabGD02-draft.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
