{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple neural network\n",
    "\n",
    "Author: Pierre Ablin\n",
    "\n",
    "In this notebook, we are going to create and train a simple neural network on the digits dataset using pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to load the data and make them into pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "# Normalize\n",
    "\n",
    "X -= X.mean(axis=0)\n",
    "X /= np.std(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "f, axes = plt.subplots(1, 3)\n",
    "for i, axe in enumerate(axes):\n",
    "    axe.imshow(X[i].reshape(8, 8))\n",
    "\n",
    "x = torch.tensor(X_train).float()\n",
    "y = torch.tensor(y_train).long()\n",
    "n, p = x.shape\n",
    "x_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the network\n",
    "\n",
    "We will work with a simple network with two layers (one hidden layer).\n",
    "\n",
    "The input $x$ is transformed into the output $z$ by the following operations:\n",
    "\n",
    "$$y = \\tanh(W_1x + b_1)$$\n",
    "$$z = W_2y + b_2$$\n",
    "\n",
    "**Exercise 1**: Define a function `net(x, W1, b1, W2, b2)` that implements this transform. Remember that `x` is a matrix of size $n\\times p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(x, W1, b1, W2, b2):\n",
    "    # Your code here.\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us specify the parameters of the network, `W1, b1, W2, b2`. You can chose the size of the hidden layer, but the input and output sizes are determined by the problem.\n",
    "\n",
    "**Exercise 2**: Define a set of parameters `W1, b1, W2, b2`, where you chose the size of the hidden layer. Make sure that all these parameters have their `requires_grad` flag set to true, so that we can compute the gradient with respect to them.\n",
    "\n",
    "In order to check that eveything works, compute `net(x, W1, b1, W2, b2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = # What you want\n",
    "input_size = # Determined by the problem size\n",
    "output_size = # Determined by the problem size\n",
    "\n",
    "W1 = # Your code here\n",
    "b1 = # Your code here\n",
    "W2 = # Your code here\n",
    "b2 = # Your code here\n",
    "\n",
    "parameters = (W1, b1, W2, b2)\n",
    "\n",
    "output = net(x, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define a cost function. We will use the classical cross entropy loss. It is imported from pytorch in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the gradient with respect to the parameters $W_1, W_2, b_1, b_2$, we will tell pytorch that we need to accumulate gradients by settings their `requires_grad` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in parameters:\n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**: Compute the current loss of the network, and then back-propagate to compute the gradient with respect to the parameters. Check the gradient with respect to W1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = net(x, W1, b1, W2, b2)\n",
    "loss = # Your code here\n",
    "print(loss.item())\n",
    "# Back propagate through the network here\n",
    "print(W1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost ready to train our network!\n",
    "\n",
    "But first, we will need to compute the accuracy of the network, on the train and test set.\n",
    "\n",
    "**Exercise 4**: Define a function `accuracy(X, y, W1, b1, W2, b2)` that computes the accuracy of the network on the dataset `x`with true labels `y`. Remember that the predicted class at the output of the network is computed as the argmaximum of the output. Compute the current accuracy of the network on the train set. Is it normal ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(X, y, W1, b1, W2, b2):\n",
    "    # Your code here\n",
    "    return\n",
    "\n",
    "accuracy(x, y, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network\n",
    "\n",
    "We are now ready to train the network, using back-propagation and stochastic gradient descent.\n",
    "First, we define the number of iterations of the algorithm, the step size, and the batch size. We also reinitialize the weights. Finally, we will store the train and test accuracy during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1000\n",
    "step_size = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "test_list = []\n",
    "train_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**: Complete the following training list, so that each parameter is updated at each iteration.\n",
    "\n",
    "Remember that at each iteration, you should:\n",
    "* compute the output of the network with respect to the batch\n",
    "* Compute the loss, and backpropagate\n",
    "* Update each parameter with gradient descent\n",
    "* Refresh the gradient of each parameter. To do so, you can do:\n",
    "\n",
    "```\n",
    "parameter.grad.data.zero_()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(n_iter):\n",
    "    batch_idx = torch.randperm(n)[:batch_size]\n",
    "    x_batch = x[batch_idx]\n",
    "    y_batch = y[batch_idx]\n",
    "    # Your code here: compute the loss, and backpropagate\n",
    "    with torch.no_grad():\n",
    "        for parameter in parameters:\n",
    "            # Your code here: update the parameters with SGD, and refresh their gradients\n",
    "    if i % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            train_acc = accuracy(x, y, W1, b1, W2, b2)\n",
    "            test_acc = accuracy(x_test, y_test, W1, b1, W2, b2)\n",
    "        test_list.append(test_acc)\n",
    "        train_list.append(train_acc)\n",
    "        print('Iteration {} Train loss: {:1.3f} Train acc: {:1.3f} Test acc {:1.3f}'.format(i, loss.item(), train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6**: Display the learning curves. You can then play with the network and training parameters:\n",
    "what happens when you change the learning rate, the number of hidden sizes, etc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_list, label='test')\n",
    "plt.plot(train_list, label='train')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Iterations')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
