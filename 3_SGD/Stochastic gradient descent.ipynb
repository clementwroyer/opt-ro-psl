{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent\n",
    "\n",
    "Author: Pierre Ablin\n",
    "   \n",
    "## General framework\n",
    "\n",
    "Consider the task of learning a map $x\\to y$ from a training dataset $(x_1, y_1), \\dots, (x_n, y_n)$. To do so, we use a parametrized model $f_{w}(x)$ that tries to predict $y$. Here, $w$ are the parameters. The parameters are learned by doing *empirical risk minimization*:\n",
    "$$\n",
    "w\\in\\arg\\min F(w) = \\frac1n\\sum_{i=1}^n f_i(w), \\enspace \\text{with} \\enspace f_i(w) = \\ell(f_w(x_i), y_i).\n",
    "$$\n",
    "Here, $\\ell$ is a function that measures a discrepancy in the space of $y$.\n",
    "\n",
    "## Examples\n",
    "For instance, when doing regression, i.e. when $y\\in\\mathbb{R}$ we can use $\\ell(y', y) = \\frac12(y - y')^2$. When doing binary classification, i.e. when $y\\in \\pm 1$, we can use the logistic loss\n",
    "$\\ell(y', y) = \\log(1+\\exp(-y'y))$.\n",
    "\n",
    "When the model $f_w$ is linear, we are doing linear regression / classification: $f_w(x) = \\langle w, x\\rangle$.\n",
    "\n",
    "## Sum structure\n",
    "\n",
    "We want to minimize $F$, which has a sum structure. We can for instance use gradient descent, starting from $w^0$:\n",
    "\n",
    "For $t = 0\\dots T$:\n",
    "$$\n",
    "w^{t+1} = w^t - \\rho \\nabla F(w^t) = w^t  - \\frac{\\rho}{n}\\sum_{i=1}^n \\nabla f_i(w^t).\n",
    "$$\n",
    "\n",
    "However, doing one step of gradient descent requires computing all the $\\nabla f_i(w^t)$, which is extremely costly when $n$ is large. This is the case in many areas of machine learning, where the datasets are big, like image processing or natural language processing.\n",
    "\n",
    "## Stochastic gradient descent (SGD)\n",
    "\n",
    "An appealing alternative is to use the gradient coming from only one of the $f_i$, where $i$ is chosen at random at each iteration\n",
    "\n",
    "\n",
    "For $t = 0\\dots T$:\n",
    "$$\n",
    "w^{t+1} = w^t - \\rho \\nabla f_i(w^t), \\enspace i \\sim U(1, n)\n",
    "$$\n",
    "This algorithm is called Stochastic Gradient Descent (SGD). It is one of the most important optimization algorithm in datascience, since it is the main algorithm to train neural networks.\n",
    "\n",
    "\n",
    "**Question 0**: Imagine that we duplicate the training set, so that now we have a dataset of size $2n$ with copies of the original dataset. What happens for gradient descent? What happens for SGD?\n",
    "## Batch size\n",
    "\n",
    "In order to leverage the parallelism offered by the computing hardware, it is often more efficient to use a mini-batch rather than a single stochastic gradient: we compute the average of $b$ gradients, where $b$ is the batch size.\n",
    "\n",
    "\n",
    "For $t = 0\\dots T$:\n",
    "$$\n",
    "w^{t+1} = w^t - \\frac{\\rho}{b}\\sum_{j=1}^b \\nabla f_{i^j}(w^t), \\enspace (i^j) \\sim P(b, n)\n",
    "$$\n",
    "where $P(b, n)$ is the uniform distribution of sets of $b$ integers in $\\{1, n\\}$.\n",
    "\n",
    "Like this, taking $b=1$ recoves SGD, and $b= n$ recovers gradient descent.\n",
    "\n",
    "We will implement it for the ridge regression problem.\n",
    "\n",
    "## Ridge regression\n",
    "\n",
    "For ridge regression, we have \n",
    "\n",
    "$$\n",
    "F(w) = \\frac1n\\|Xw - y\\|^2 + \\lambda \\|w\\|^2\n",
    "$$\n",
    "\n",
    "where $X$ is the data matrix and $y$ the target vector.\n",
    "\n",
    "\n",
    "**Question 1**: For this problem, what is $f_i(w)$?\n",
    "\n",
    "Below we implement SGD with mini-batch for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "n = 1000\n",
    "p = 20\n",
    "\n",
    "X = np.random.randn(n, p)\n",
    "y = np.random.randn(n)\n",
    "lbda = 0.1\n",
    "\n",
    "def ridge_loss(w, X, y, lbda):\n",
    "    n, p = X.shape\n",
    "    res = np.dot(X, w) - y\n",
    "    return 0.5 * (np.dot(res, res) / n + lbda * np.dot(w, w))\n",
    "\n",
    "def ridge_solution(X, y, lbda):\n",
    "    n, p = X.shape\n",
    "    K = np.dot(X.T, X) + lbda * n * np.eye(p)\n",
    "    return np.linalg.solve(K, np.dot(X.T, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_sgd(w0, X, y, lbda, batch_size, step_size, n_iters=10000):\n",
    "    w = w0.copy()\n",
    "    w_list = []\n",
    "    for i in range(n_iters):\n",
    "        # Your code here: sample a mini-batch, compute the gradient, etc...\n",
    "        w_list.append(w.copy())\n",
    "    return w, w_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, w_list = ridge_sgd(np.zeros(p), X, y, lbda, 10, .001)\n",
    "w_star = ridge_solution(X, y, lbda)\n",
    "dists = [np.linalg.norm(w - w_star) for w in w_list]\n",
    "plt.semilogy(dists)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**: Investigate the behavior of SGD with different batch size and step size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
