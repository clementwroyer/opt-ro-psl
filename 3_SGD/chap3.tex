\documentclass[10pt,serif]{beamer}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[round]{natbib}

\usepackage[most,skins,theorems]{tcolorbox}
\usepackage{color}
\definecolor{oxford_blue}{RGB}{14,31,71}

\usetheme[progressbar=frametitle]{metropolis}
\usepackage{eso-pic}
\usepackage{booktabs}
\usepackage[scale=2]{ccicons}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\T}{\mathrm{T}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\D}{\mathrm{D}}
\newcommand{\calL}{\mathcal{L}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\calM}{\mathcal{M}}
\newcommand{\calD}{\mathcal{D}}
\newcommand{\calC}{\mathcal{C}}
\newcommand{\Sbb}{\mathbb{S}}
\newcommand{\Cbb}{\mathbb{C}}
\newcommand{\Nbb}{\mathbb{N}}
\newcommand{\Zbb}{\mathbb{Z}}
\newcommand{\Rbb}{\mathbb{R}}
\newcommand{\Rn}{\mathbb{R}^n}
\newcommand{\Rd}{\mathbb{R}^d}
\newcommand{\Rm}{\mathbb{R}^m}
\newcommand{\Grass}{\text{Grass}}
\newcommand{\red}{\textcolor{red}}
\newcommand{\orange}{\textcolor{orange}}
\newcommand{\blue}{\textcolor{blue}}
\newcommand{\dr}{\partial}
\newcommand{\Lg}{\mathrm{L}_g}
\newcommand{\txt}{\text}
\newcommand{\Nrm}{\mathrm{N}}
\newcommand{\grad}{\mathrm{grad}}
\newcommand{\Hess}{\mathrm{Hess}}
\newcommand{\hess}{\nabla^2}
\newcommand{\inner}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\Id}{\mathrm{Id}}
\newcommand{\I}{\mathrm{I}}
\newcommand{\sigmamin}{\sigma_\mathrm{min}}
\newcommand{\sigmamax}{\sigma_\mathrm{max}}
\newcommand{\sigmabar}{\underline{\sigma}}
\newcommand{\argmin}{\operatorname{argmin}}
\newcommand{\spann}{\operatorname{span}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\trace}{\mathrm{trace}}
\newcommand{\transpose}{^\top}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\minimize}{\operatorname{minimize}}
\newcommand{\lambdamin}{\lambda_\mathrm{min}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\dis}{\displaystyle}
\newcommand{\td}{\todo[inline]}
\newcommand{\ttt}{\texttt}
\newcommand{\nl}{\newline}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\mycaption}[1]{
  {
    \smaller
    \emph{#1}
  }
}

\DeclareMathAlphabet{\mathantt}{OT1}{antt}{li}{it}

\usepackage{tabu}
%\newtheorem{definition}	{Definition}[chapter]
%\newtheorem{example}	{Example}[chapter]
\newtheorem{proposition}	{Proposition}%[chapter]
%\newtheorem{theorem}	{Theorem}[chapter]
\newtheorem{hypothesis}	{Hypothesis}%[chapter]
\newtheorem{assumption}	{Assumption}


\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{caption}
\usepackage{algpseudocode}
\usepackage{todonotes}
\usepackage{tikz}
\usepackage{epstopdf}
\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}
\usepackage{listings}

\usepackage{xspace}
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}

\title{Partie 3: Methode du gradient stochastique}  
\date{12 octobre 2022}
\author{Florentin Goyens}

\begin{document}


\maketitle

\begin{frame}{What is a learning problem ? }

Intro 

\end{frame}


\begin{frame}{Learning and training set}
\begin{itemize}
\item Learning is an optimization problem over the training set, in the hope that it generalizes to unseen data.  
\end{itemize}
\end{frame}

\begin{frame}{The finite sum minimization problem}
\begin{align*}
 \minimize_{x\in \Rd} &\sum_{i=1}^n (h(x_i) - y_i)^2 =  \sum_{i=1}^n  f_i(x)\\
\end{align*}

\begin{align*}
F(x) &= \sum_{i=1}^n  f_i(x)\\
\end{align*}
\begin{align*}
\nabla F(x) &= \sum_{i=1}^n  \nabla f_i(x)\\
\end{align*}
\end{frame}
\begin{frame}{Title}
\begin{align*}
\minimize_{x\in \Rd} F(x) = \sum_{i=1}^n  f_i(x)
\end{align*}
Gradient descent method:
\begin{align*}
x_{k+1} = x_k - \gamma \nabla F(x_k)
\end{align*}
Stochastic gradient method: 
\begin{align*}
x_{k+1} = x_k - \gamma \nabla f_i(x_k)
\end{align*}
\end{frame}

\end{document}