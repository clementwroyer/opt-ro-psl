{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple neural network\n",
    "\n",
    "\n",
    "In this notebook, we are going to create and train a simple neural network on the digits dataset using pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to load the data and make them into pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "# Normalize\n",
    "\n",
    "X -= X.mean(axis=0)\n",
    "X /= np.std(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "f, axes = plt.subplots(1, 3)\n",
    "for i, axe in enumerate(axes):\n",
    "    axe.imshow(X[i].reshape(8, 8))\n",
    "\n",
    "x = torch.tensor(X_train).float()\n",
    "y = torch.tensor(y_train).long()\n",
    "n, p = x.shape\n",
    "x_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the network\n",
    "\n",
    "We will work with a simple network with two layers (one hidden layer).\n",
    "\n",
    "The input $x$ is transformed into the output $z$ by the following operations:\n",
    "\n",
    "$$y = \\tanh(W_1x + b_1)$$\n",
    "$$z = W_2y + b_2$$\n",
    "\n",
    "**Exercise 1**: Define a function `net(x, W1, b1, W2, b2)` that implements this transform. Remember that `x` is a matrix of size $n\\times p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(x, W1, b1, W2, b2):\n",
    "    y = torch.mm(x, W1.t()) + b1\n",
    "    y = torch.tanh(y)\n",
    "    z =  torch.mm(y, W2.t()) + b2\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us specify the parameters of the network, `W1, b1, W2, b2`. You can chose the size of the hidden layer, but the input and output sizes are determined by the problem.\n",
    "\n",
    "**Exercise 2**: Define a set of parameters `W1, b1, W2, b2`, where you chose the size of the hidden layer. Make sure that all these parameters have their `requires_grad` flag set to true, so that we can compute the gradient with respect to them.\n",
    "\n",
    "In order to check that eveything works, compute `net(x, W1, b1, W2, b2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 20\n",
    "input_size = 64\n",
    "output_size = 10\n",
    "\n",
    "W1 = torch.randn(hidden_size, input_size)\n",
    "b1 = torch.randn(hidden_size)\n",
    "W2 = torch.randn(output_size, hidden_size)\n",
    "b2 = torch.randn(output_size)\n",
    "\n",
    "parameters = (W1, b1, W2, b2)\n",
    "\n",
    "output = net(x, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define a cost function. We will use the classical cross entropy loss. It is imported from pytorch in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the gradient with respect to the parameters $W_1, W_2, b_1, b_2$, we will tell pytorch that we need to accumulate gradients by settings their `requires_grad` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in parameters:\n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**: Compute the current loss of the network, and then back-propagate to compute the gradient with respect to the parameters. Check the gradient with respect to W1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.224804878234863\n",
      "tensor([[ 0.0000,  0.0067,  0.0400,  ..., -0.0020,  0.0289,  0.0121],\n",
      "        [ 0.0000,  0.0095,  0.0866,  ...,  0.0346,  0.0372,  0.0115],\n",
      "        [ 0.0000, -0.0010,  0.0069,  ..., -0.0146, -0.0225, -0.0061],\n",
      "        ...,\n",
      "        [ 0.0000, -0.0008, -0.0064,  ..., -0.0344, -0.0089, -0.0003],\n",
      "        [ 0.0000,  0.0011,  0.0177,  ...,  0.0147, -0.0167, -0.0071],\n",
      "        [ 0.0000,  0.0046,  0.0131,  ..., -0.0104, -0.0080, -0.0039]])\n"
     ]
    }
   ],
   "source": [
    "output = net(x, W1, b1, W2, b2)\n",
    "loss = cross_entropy(output, y)\n",
    "print(loss.item())\n",
    "loss.backward()\n",
    "print(W1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost ready to train our network!\n",
    "\n",
    "But first, we will need to compute the accuracy of the network, on the train and test set.\n",
    "\n",
    "**Exercise 4**: Define a function `accuracy(X, y, W1, b1, W2, b2)` that computes the accuracy of the network on the dataset `x`with true labels `y`. Remember that the predicted class at the output of the network is computed as the argmaximum of the output. Compute the current accuracy of the network on the train set. Is it normal ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0935412026726058"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(X, y, W1, b1, W2, b2):\n",
    "    output = net(X, W1, b1, W2, b2)\n",
    "    pred = output.argmax(axis=1)\n",
    "    return (pred == y).sum().item() / len(y)\n",
    "\n",
    "\n",
    "accuracy(x, y, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network\n",
    "\n",
    "We are now ready to train the network, using back-propagation and stochastic gradient descent.\n",
    "First, we define the number of iterations of the algorithm, the step size, and the batch size. We also reinitialize the weights. Finally, we will store the train and test accuracy during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1000\n",
    "step_size = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "test_list = []\n",
    "train_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**: Complete the following training list, so that each parameter is updated at each iteration.\n",
    "\n",
    "Remember that at each iteration, you should:\n",
    "* compute the output of the network with respect to the batch\n",
    "* Compute the loss, and backpropagate\n",
    "* Update each parameter with gradient descent\n",
    "* Refresh the gradient of each parameter. To do so, you can do:\n",
    "\n",
    "```\n",
    "parameter.grad.data.zero_()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Train loss: 7.732 Train acc: 0.098 Test acc 0.109\n",
      "Iteration 10 Train loss: 5.248 Train acc: 0.149 Test acc 0.147\n",
      "Iteration 20 Train loss: 5.237 Train acc: 0.215 Test acc 0.211\n",
      "Iteration 30 Train loss: 3.281 Train acc: 0.281 Test acc 0.282\n",
      "Iteration 40 Train loss: 2.878 Train acc: 0.367 Test acc 0.340\n",
      "Iteration 50 Train loss: 2.426 Train acc: 0.445 Test acc 0.400\n",
      "Iteration 60 Train loss: 1.881 Train acc: 0.512 Test acc 0.442\n",
      "Iteration 70 Train loss: 1.846 Train acc: 0.549 Test acc 0.473\n",
      "Iteration 80 Train loss: 1.467 Train acc: 0.581 Test acc 0.518\n",
      "Iteration 90 Train loss: 1.305 Train acc: 0.607 Test acc 0.544\n",
      "Iteration 100 Train loss: 1.540 Train acc: 0.636 Test acc 0.573\n",
      "Iteration 110 Train loss: 1.066 Train acc: 0.659 Test acc 0.589\n",
      "Iteration 120 Train loss: 1.108 Train acc: 0.682 Test acc 0.607\n",
      "Iteration 130 Train loss: 0.895 Train acc: 0.699 Test acc 0.618\n",
      "Iteration 140 Train loss: 1.007 Train acc: 0.721 Test acc 0.640\n",
      "Iteration 150 Train loss: 0.708 Train acc: 0.737 Test acc 0.649\n",
      "Iteration 160 Train loss: 0.798 Train acc: 0.752 Test acc 0.660\n",
      "Iteration 170 Train loss: 0.932 Train acc: 0.761 Test acc 0.671\n",
      "Iteration 180 Train loss: 0.611 Train acc: 0.772 Test acc 0.682\n",
      "Iteration 190 Train loss: 0.771 Train acc: 0.782 Test acc 0.700\n",
      "Iteration 200 Train loss: 0.655 Train acc: 0.793 Test acc 0.704\n",
      "Iteration 210 Train loss: 0.650 Train acc: 0.803 Test acc 0.713\n",
      "Iteration 220 Train loss: 0.841 Train acc: 0.814 Test acc 0.722\n",
      "Iteration 230 Train loss: 0.566 Train acc: 0.821 Test acc 0.722\n",
      "Iteration 240 Train loss: 0.601 Train acc: 0.833 Test acc 0.729\n",
      "Iteration 250 Train loss: 0.454 Train acc: 0.838 Test acc 0.733\n",
      "Iteration 260 Train loss: 0.534 Train acc: 0.843 Test acc 0.736\n",
      "Iteration 270 Train loss: 0.519 Train acc: 0.849 Test acc 0.744\n",
      "Iteration 280 Train loss: 0.396 Train acc: 0.855 Test acc 0.756\n",
      "Iteration 290 Train loss: 0.340 Train acc: 0.857 Test acc 0.756\n",
      "Iteration 300 Train loss: 0.418 Train acc: 0.860 Test acc 0.762\n",
      "Iteration 310 Train loss: 0.283 Train acc: 0.863 Test acc 0.776\n",
      "Iteration 320 Train loss: 0.596 Train acc: 0.867 Test acc 0.773\n",
      "Iteration 330 Train loss: 0.285 Train acc: 0.877 Test acc 0.782\n",
      "Iteration 340 Train loss: 0.467 Train acc: 0.878 Test acc 0.782\n",
      "Iteration 350 Train loss: 0.336 Train acc: 0.880 Test acc 0.787\n",
      "Iteration 360 Train loss: 0.434 Train acc: 0.883 Test acc 0.787\n",
      "Iteration 370 Train loss: 0.334 Train acc: 0.886 Test acc 0.789\n",
      "Iteration 380 Train loss: 0.582 Train acc: 0.885 Test acc 0.789\n",
      "Iteration 390 Train loss: 0.360 Train acc: 0.887 Test acc 0.793\n",
      "Iteration 400 Train loss: 0.388 Train acc: 0.889 Test acc 0.784\n",
      "Iteration 410 Train loss: 0.228 Train acc: 0.889 Test acc 0.793\n",
      "Iteration 420 Train loss: 0.425 Train acc: 0.892 Test acc 0.798\n",
      "Iteration 430 Train loss: 0.301 Train acc: 0.895 Test acc 0.807\n",
      "Iteration 440 Train loss: 0.219 Train acc: 0.898 Test acc 0.807\n",
      "Iteration 450 Train loss: 0.305 Train acc: 0.899 Test acc 0.811\n",
      "Iteration 460 Train loss: 0.341 Train acc: 0.903 Test acc 0.807\n",
      "Iteration 470 Train loss: 0.392 Train acc: 0.903 Test acc 0.807\n",
      "Iteration 480 Train loss: 0.326 Train acc: 0.909 Test acc 0.811\n",
      "Iteration 490 Train loss: 0.215 Train acc: 0.909 Test acc 0.811\n",
      "Iteration 500 Train loss: 0.294 Train acc: 0.909 Test acc 0.813\n",
      "Iteration 510 Train loss: 0.413 Train acc: 0.910 Test acc 0.818\n",
      "Iteration 520 Train loss: 0.213 Train acc: 0.912 Test acc 0.816\n",
      "Iteration 530 Train loss: 0.265 Train acc: 0.912 Test acc 0.816\n",
      "Iteration 540 Train loss: 0.185 Train acc: 0.914 Test acc 0.816\n",
      "Iteration 550 Train loss: 0.152 Train acc: 0.915 Test acc 0.816\n",
      "Iteration 560 Train loss: 0.213 Train acc: 0.916 Test acc 0.816\n",
      "Iteration 570 Train loss: 0.291 Train acc: 0.918 Test acc 0.813\n",
      "Iteration 580 Train loss: 0.357 Train acc: 0.919 Test acc 0.818\n",
      "Iteration 590 Train loss: 0.377 Train acc: 0.924 Test acc 0.818\n",
      "Iteration 600 Train loss: 0.296 Train acc: 0.927 Test acc 0.818\n",
      "Iteration 610 Train loss: 0.337 Train acc: 0.928 Test acc 0.822\n",
      "Iteration 620 Train loss: 0.318 Train acc: 0.928 Test acc 0.824\n",
      "Iteration 630 Train loss: 0.237 Train acc: 0.928 Test acc 0.827\n",
      "Iteration 640 Train loss: 0.135 Train acc: 0.930 Test acc 0.827\n",
      "Iteration 650 Train loss: 0.270 Train acc: 0.932 Test acc 0.824\n",
      "Iteration 660 Train loss: 0.145 Train acc: 0.933 Test acc 0.822\n",
      "Iteration 670 Train loss: 0.159 Train acc: 0.932 Test acc 0.822\n",
      "Iteration 680 Train loss: 0.281 Train acc: 0.933 Test acc 0.827\n",
      "Iteration 690 Train loss: 0.348 Train acc: 0.935 Test acc 0.827\n",
      "Iteration 700 Train loss: 0.160 Train acc: 0.935 Test acc 0.827\n",
      "Iteration 710 Train loss: 0.163 Train acc: 0.935 Test acc 0.827\n",
      "Iteration 720 Train loss: 0.150 Train acc: 0.936 Test acc 0.827\n",
      "Iteration 730 Train loss: 0.225 Train acc: 0.938 Test acc 0.831\n",
      "Iteration 740 Train loss: 0.234 Train acc: 0.938 Test acc 0.833\n",
      "Iteration 750 Train loss: 0.190 Train acc: 0.938 Test acc 0.831\n",
      "Iteration 760 Train loss: 0.151 Train acc: 0.937 Test acc 0.827\n",
      "Iteration 770 Train loss: 0.313 Train acc: 0.938 Test acc 0.831\n",
      "Iteration 780 Train loss: 0.367 Train acc: 0.941 Test acc 0.824\n",
      "Iteration 790 Train loss: 0.089 Train acc: 0.940 Test acc 0.829\n",
      "Iteration 800 Train loss: 0.175 Train acc: 0.940 Test acc 0.829\n",
      "Iteration 810 Train loss: 0.221 Train acc: 0.941 Test acc 0.829\n",
      "Iteration 820 Train loss: 0.172 Train acc: 0.943 Test acc 0.829\n",
      "Iteration 830 Train loss: 0.164 Train acc: 0.945 Test acc 0.831\n",
      "Iteration 840 Train loss: 0.175 Train acc: 0.945 Test acc 0.831\n",
      "Iteration 850 Train loss: 0.118 Train acc: 0.944 Test acc 0.833\n",
      "Iteration 860 Train loss: 0.261 Train acc: 0.945 Test acc 0.833\n",
      "Iteration 870 Train loss: 0.221 Train acc: 0.944 Test acc 0.833\n",
      "Iteration 880 Train loss: 0.192 Train acc: 0.947 Test acc 0.833\n",
      "Iteration 890 Train loss: 0.209 Train acc: 0.948 Test acc 0.836\n",
      "Iteration 900 Train loss: 0.210 Train acc: 0.950 Test acc 0.840\n",
      "Iteration 910 Train loss: 0.071 Train acc: 0.952 Test acc 0.842\n",
      "Iteration 920 Train loss: 0.192 Train acc: 0.951 Test acc 0.842\n",
      "Iteration 930 Train loss: 0.158 Train acc: 0.951 Test acc 0.844\n",
      "Iteration 940 Train loss: 0.130 Train acc: 0.953 Test acc 0.844\n",
      "Iteration 950 Train loss: 0.165 Train acc: 0.955 Test acc 0.844\n",
      "Iteration 960 Train loss: 0.134 Train acc: 0.955 Test acc 0.842\n",
      "Iteration 970 Train loss: 0.154 Train acc: 0.956 Test acc 0.844\n",
      "Iteration 980 Train loss: 0.147 Train acc: 0.955 Test acc 0.844\n",
      "Iteration 990 Train loss: 0.150 Train acc: 0.955 Test acc 0.853\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_iter):\n",
    "    batch_idx = torch.randperm(n)[:batch_size]\n",
    "    x_batch = x[batch_idx]\n",
    "    y_batch = y[batch_idx]\n",
    "    output = net(x_batch, W1, b1, W2, b2)\n",
    "    loss = cross_entropy(output, y_batch)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        for parameter in parameters:\n",
    "            parameter -= step_size * parameter.grad\n",
    "            parameter.grad.data.zero_()\n",
    "    if i % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            train_acc = accuracy(x, y, W1, b1, W2, b2)\n",
    "            test_acc = accuracy(x_test, y_test, W1, b1, W2, b2)\n",
    "        test_list.append(test_acc)\n",
    "        train_list.append(train_acc)\n",
    "        print('Iteration {} Train loss: {:1.3f} Train acc: {:1.3f} Test acc {:1.3f}'.format(i, loss.item(), train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6**: Display the learning curves. You can then play with the network and training parameters:\n",
    "what happens when you change the learning rate, the number of hidden sizes, etc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f41e4727990>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_list, label='test')\n",
    "plt.plot(train_list, label='train')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Iterations')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
