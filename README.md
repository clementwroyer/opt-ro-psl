# Formation "Optimisation-RO" pour le certificat "Chef de Projet IA" de l'université Paris Dauphine-PSL.

### Enseignants : Pierre Ablin (CNRS et Université Paris Dauphine-PSL), Clément Royer (Université Paris Dauphine-PSL)

*Pré-requis :  Notions d'algèbre linéaire élémentaire et de programmation.*
 

## Programme des séances

1. **Bases de l'optimisation lisse/douce** Rappels d'analyse, principes de base en optimisation continue; méthodes de gradient et Newton, problèmes aux moindres carrés.
2. **Programmation convexe** Formulations convexes, dualité et conditions de KKT, éléments d'analyse convexe, algorithmes et librairies pour l'optimisation convexe.
3. **Méthodes stochastiques à grande échelle 1/2** Algorithme du gradient stochastique, résultats théoriques et choix du pas, techniques de réduction de variance.
4. **Méthodes stochastiques à grande échelle 2/2** Implémentation du gradient stochastique dans les réseaux de neurones, différentiation automatique, mise à l'échelle, choix des hyperparamètres.
5. **Optimisation sans dérivées** Terminologie et définitions, méthodes de recherche directe et connections avec les bandits, méthodes basées sur des modèles, processus gaussiens et optimisation bayésienne.


### Bibliographie :

- **Convex Optimization.** S. Boyd and L. Vandenberghe, Cambridge University Press, 2004.
- **Numerical Optimization, Second Edition.** J. Nocedal and S. J. Wright, Springer-Verlag, 2006.
- **Optimization for Data Analysis**. S. J. Wright, in *The Mathematics of Data*, IAS/Park City Mathematics Series, 2016.
- **Optimization methods for large-scale machine learning**. L. Bottou, F. E. Curtis and J. Nocedal, *SIAM Review*, 2018.
